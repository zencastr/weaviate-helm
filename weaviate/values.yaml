externalName: weaviate.stage.az.zencastr.com
image:
  # registry where weaviate image is stored
  registry: docker.io
  # Tag of weaviate image to deploy
  # Note: We strongly recommend you overwrite this value in your own values.yaml.
  # Otherwise a mere upgrade of the chart could lead to an unexpected upgrade
  # of weaviate. In accordance with Infra-as-code, you should pin this value
  # down and only change it if you explicitly want to upgrade the Weaviate
  # version.
  tag: 1.7.2
  repo: semitechnologies/weaviate

# manually scale horizontal replicas of weaviate.
# Also note: https://github.com/semi-technologies/weaviate/issues/725
replicas: 1
resources:
  requests:
    cpu: '500m'
    memory: '300Mi'
  limits:
    cpu: '1000m'
    memory: '1Gi'

# The Persitent Volume Claim settings for Weaviate. If there's a
# storage.fullnameOverride field set, then the default pvc will not be
# created, instead the one defined in fullnameOverride will be used
storage:
  size: 100Gi

# The service controls how weaviate is exposed to the outside world. If you
# don't want a public load balancer, you can also choose 'ClusterIP' to make
# weaviate only accessible within your cluster.
service:
  name: weaviate
  type: ClusterIP

# Weaviate Config
#
# The following settings allow you to customize Weaviate to your needs, for
# example set authentication and authorization optins. See weaviate docs
# (https://www.semi.technology/documentation/weaviate/current/) for all
# configuration.
authentication:
  anonymous_access:
    enabled: true
authorization:
  admin_list:
    enabled: false
query_defaults:
  limit: 100
debug: false

# modules are extensions to Weaviate, they can be used to support various
# ML-models, but also other features unrelated to model inference.
# An inference/vectorizer module is not required, you can also run without any
# modules and import your own vectors.
modules:

  # The text2vec-contextionary module uses a fastText-based vector-space to
  # derive vector embeddings for your objects. It is very efficient on CPUs,
  # but in some situations it cannot reach the same level of accuraccy as
  # transformers-based models.
  text2vec-contextionary:
    # disable if you want to use transformers or import or own vectors
    enabled: false

    # The configuration below is ignored if enabled==false
    fullnameOverride: contextionary
    tag: en0.16.0-v1.0.0
    repo: semitechnologies/contextionary
    registry: docker.io
    replicas: 1
    envconfig:
      occurrence_weight_linear_factor: 0.75
      neighbor_occurrence_ignore_percentile: 5
      enable_compound_splitting: false
      extensions_storage_mode: weaviate
    resources:
      requests:
        cpu: '500m'
        memory: '500Mi'
      limits:
        cpu: '1000m'
        memory: '5000Mi'

  # The text2vec-transformers modules uses neural networks, such as BERT,
  # DistilBERT, etc. to dynamically compute vector embeddings based on the
  # sentence's context. It is very slow on CPUs and should run with
  # CUDA-enabled GPUs for optimal performance.
  text2vec-transformers:

    # enable if you want to use transformers instead of the
    # text2vec-contextionary module
    enabled: true

    # The configuration below is ignored if enabled==false

    # replace with model of choice, see
    # https://www.semi.technology/developers/weaviate/current/modules/text2vec-transformers.html
    # for all supported models or build your own container.
    tag: sentence-transformers-multi-qa-mpnet-base-cos-v1
    repo: semitechnologies/transformers-inference
    registry: docker.io
    replicas: 1
    fullnameOverride: transformers-inference
    envconfig:
      # enable for CUDA support. Your K8s cluster needs to be configured
      # accordingly and you need to explicilty set GPU requests & limits below
      enable_cuda: true

      # only used when cuda is enabled
      nvidia_visible_devices: all

      # only used when cuda is enabled
      ld_library_path: /usr/local/nvidia/lib64

    resources:
      requests:
        cpu: '1000m'
        memory: '3000Mi'

        # enable if running with CUDA support
        nvidia.com/gpu: 1
      limits:
        cpu: '1000m'
        memory: '5000Mi'

        # enable if running with CUDA support
        nvidia.com/gpu: 1

  # The qna-transformers module uses neural networks, such as BERT,
  # DistilBERT, to find an aswer in text to a given question
  qna-transformers:
    enabled: false
    tag: bert-large-uncased-whole-word-masking-finetuned-squad-34d66b1
    repo: semitechnologies/qna-transformers
    registry: docker.io
    replicas: 1
    fullnameOverride: qna-transformers
    envconfig:
      # enable for CUDA support. Your K8s cluster needs to be configured
      # accordingly and you need to explicilty set GPU requests & limits below
      enable_cuda: false

      # only used when cuda is enabled
      nvidia_visible_devices: all

      # only used when cuda is enabled
      ld_library_path: /usr/local/nvidia/lib64

    resources:
      requests:
        cpu: '1000m'
        memory: '3000Mi'

        # enable if running with CUDA support
        # nvidia.com/gpu: 1
      limits:
        cpu: '1000m'
        memory: '5000Mi'

        # enable if running with CUDA support
        # nvidia.com/gpu: 1

  # The img2vec-neural module uses neural networks, to generate
  # a vector respresentation of the image
  img2vec-neural:
    enabled: false
    tag: resnet50
    repo: semitechnologies/img2vec-pytorch
    registry: docker.io
    replicas: 1
    fullnameOverride: img2vec-neural
    envconfig:
      # enable for CUDA support. Your K8s cluster needs to be configured
      # accordingly and you need to explicilty set GPU requests & limits below
      enable_cuda: false

      # only used when cuda is enabled
      nvidia_visible_devices: all

      # only used when cuda is enabled
      ld_library_path: /usr/local/nvidia/lib64

    resources:
      requests:
        cpu: '1000m'
        memory: '3000Mi'

        # enable if running with CUDA support
        # nvidia.com/gpu: 1
      limits:
        cpu: '1000m'
        memory: '5000Mi'

        # enable if running with CUDA support
        # nvidia.com/gpu: 1

  # The text-spellcheck module uses spellchecker library to check
  # misspellings in a given text
  text-spellcheck:
    enabled: false
    tag: pyspellchecker-en
    repo: semitechnologies/text-spellcheck-model
    registry: docker.io
    replicas: 1
    fullnameOverride: text-spellcheck
    envconfig:
      # enable for CUDA support. Your K8s cluster needs to be configured
      # accordingly and you need to explicilty set GPU requests & limits below
      enable_cuda: false

      # only used when cuda is enabled
      nvidia_visible_devices: all

      # only used when cuda is enabled
      ld_library_path: /usr/local/nvidia/lib64

    resources:
      requests:
        cpu: '1000m'
        memory: '3000Mi'

        # enable if running with CUDA support
        # nvidia.com/gpu: 1
      limits:
        cpu: '1000m'
        memory: '5000Mi'

        # enable if running with CUDA support
        # nvidia.com/gpu: 1

  # The ner-transformers module uses spellchecker library to check
  # misspellings in a given text
  ner-transformers:
    enabled: false
    tag: dbmdz-bert-large-cased-finetuned-conll03-english-0.0.2
    repo: semitechnologies/ner-transformers
    registry: docker.io
    replicas: 1
    fullnameOverride: ner-transformers
    envconfig:
      # enable for CUDA support. Your K8s cluster needs to be configured
      # accordingly and you need to explicilty set GPU requests & limits below
      enable_cuda: false

      # only used when cuda is enabled
      nvidia_visible_devices: all

      # only used when cuda is enabled
      ld_library_path: /usr/local/nvidia/lib64

    resources:
      requests:
        cpu: '1000m'
        memory: '3000Mi'

        # enable if running with CUDA support
        # nvidia.com/gpu: 1
      limits:
        cpu: '1000m'
        memory: '5000Mi'

        # enable if running with CUDA support
        # nvidia.com/gpu: 1

  # by choosing the default vectorizer module, you can tell Weaviate to always
  # use this module as the vectorizer if nothing else is specified. Can be
  # overwritten on a per-class basis.
  # set to text2vec-transformers if running with transformers instead
  default_vectorizer_module: text2vec-transformers

# It is also possible to configure authentication and authorization through a
# custom configmap The authorization and authentication values defined in
# values.yaml will be ignored when defining a custom config map.
custom_config_map:
  enabled: false
  name: 'custom-config'

# The collector proxy collects meta data over the requests.  It deploys a
# second service that, if used, will capture meta data over the incoming
# requests.  The collected data may be used to optimize the software or detect
# mallicious attacks.
collector_proxy:
  enabled: false
  tag: latest
  weaviate_enterprise_usage_collector_origin: ''
  weaviate_enterprise_token: ''
  weaviate_enterprise_project: ''
  service:
    name: 'usage-proxy'
    port: 80
    type: LoadBalancer
